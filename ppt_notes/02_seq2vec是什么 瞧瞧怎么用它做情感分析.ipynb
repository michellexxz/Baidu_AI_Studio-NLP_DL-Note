{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# paddlenlp.seq2vec是什么？快来看看如何用它完成情感分析任务\n",
    "\n",
    "**注意**\n",
    "\n",
    "建议本项目使用**GPU**环境来运行:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/767f625548714f03b105b6ccb3aa78df9080e38d329e445380f505ddec6c7042\" width=\"30%\" height=\"30%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "情感分析是自然语言处理领域一个老生常谈的任务。句子情感分析目的是为了判别说者的情感倾向，比如在某些话题上给出的的态度明确的观点，或者反映的情绪状态等。情感分析有着广泛应用，比如电商评论分析、舆情分析等。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/febb8a1478e34258953e56611ddc76cd20b412fec89845b0a4a2e6b9f8aae774\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "## paddlenlp.seq2vec\n",
    "\n",
    "句子情感分析的关键技术是如何将文本表示成一个**携带语义的文本向量**。随着深度学习技术的快速发展，目前常用的文本表示技术有LSTM，GRU，RNN等方法。\n",
    "PaddleNLP提供了一系列的文本表示技术，集成在`seq2vec`模块中。\n",
    "\n",
    "[`paddlenlp.seq2vec`](https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/paddlenlp/seq2vec) 模块的作用是将输入的序列文本，表示成一个语义向量。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bbf00931c7534ab48a5e7dff5fbc2ba3ff8d459940434628ad21e9195da5d4c6\" width=\"700\" height=\"350\" ></center>\n",
    "<br><center>图1：paddlenlp.seq2vec示意图</center></br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**`seq2vec`模块**\n",
    "\n",
    "* 输入：文本序列的Embedding Tensor，shape：(batch_size, num_token, emb_dim)\n",
    "* 输出：文本语义表征Enocded Texts Tensor，shape：(batch_sie,encoding_size)\n",
    "* 提供了`BoWEncoder`，`CNNEncoder`，`GRUEncoder`，`LSTMEncoder`，`RNNEncoder`等模型\n",
    "\t- `BoWEncoder` 是将输入序列Embedding Tensor在num_token维度上叠加，得到文本语义表征Enocded Texts Tensor。     \n",
    "    \n",
    "    - `CNNEncoder` 是将输入序列Embedding Tensor进行卷积操作，在对卷积结果进行max_pooling，得到文本语义表征Enocded Texts Tensor。   \n",
    "    \n",
    "    - `GRUEncoder` 是对输入序列Embedding Tensor进行GRU运算，在运算结果上进行pooling或者取最后一个step的隐表示，得到文本语义表征Enocded Texts Tensor。     \n",
    "    \n",
    "    - `LSTMEncoder` 是对输入序列Embedding Tensor进行LSTM运算，在运算结果上进行pooling或者取最后一个step的隐表示，得到文本语义表征Enocded Texts Tensor。   \n",
    "    \n",
    "    - `RNNEncoder` 是对输入序列Embedding Tensor进行RNN运算，在运算结果上进行pooling或者取最后一个step的隐表示，得到文本语义表征Enocded Texts Tensor。\n",
    "    \n",
    "    \n",
    "* `seq2vec`提供了许多语义表征方法，那么这些方法有什么特点呢？\n",
    "\t1. `BoWEncoder`采用Bag of Word Embedding方法，其特点是简单。但其缺点是没有考虑文本的语境，所以对文本语义的表征不足以表意。\n",
    "    2. `CNNEncoder`采用卷积操作，提取局部特征，其特点是可以共享权重。但其缺点同样只考虑了局部语义，上下文信息没有充分利用。\n",
    "\n",
    "  <center>\n",
    "    <img src=\"https://ai-studio-static-online.cdn.bcebos.com/2b2498edd83e49d3b017c4a14e1be68506349249b8a24cdaa214755fb51eadcd\" width=\"400\" height=\"150\" >\n",
    "  </center>\n",
    "  <center>\n",
    "    图2：卷积示意图\n",
    "  </center>\n",
    "  </br>\n",
    "\n",
    "\t3. `RNNEnocder`采用RNN方法，在计算下一个token语义信息时，利用上一个token语义信息作为其输入。但其缺点容易产生梯度消失和梯度爆炸。\n",
    "\n",
    "    <p align=\"center\">\n",
    "    <img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width = \"40%\" height = \"20%\"  hspace='10'/> \n",
    "    </p>\n",
    "    <center>\n",
    "      图3：RNN示意图\n",
    "    </center>\n",
    "    </br>\n",
    "\n",
    "\t4. `LSTMEnocder`采用LSTM方法，LSTM是RNN的一种变种。为了学到长期依赖关系，LSTM 中引入了门控机制来控制信息的累计速度，包括有选择地加入新的信息，并有选择地遗忘之前累计的信息。\n",
    "\n",
    "  <p align=\"center\">\n",
    "    <img src=\"https://ai-studio-static-online.cdn.bcebos.com/a5af1d93c69f422d963e094397a2f6ce978c30a26ab6480ab70d688dd1929de0\" width = \"50%\" height = \"30%\"  hspace='10'/> \n",
    "  </center>\n",
    "  <center>\n",
    "    图4：LSTM示意图\n",
    "  </center>\n",
    "  </br>\n",
    "\n",
    "\t5. `GRUEncoder`采用GRU方法，GRU也是RNN的一种变种。一个LSTM单元有四个输入 ，因而参数是RNN的四倍，带来的结果是训练速度慢。GRU对LSTM进行了简化，在不影响效果的前提下加快了训练速度。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/fc848bc2cb494b40ae42af892b756f5888770320a1fa42348cec10d3df64ee2f\" width = \"40%\" height = \"25%\"  hspace='10'/> \n",
    "  <br />\n",
    "</p><br><center>图5：GRU示意图</center></br>\n",
    "    \n",
    "    \n",
    "关于CNN、LSTM、GRU、RNN等更多信息参考：\n",
    "* Understanding LSTM Networks: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling:[https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)\n",
    "* A Critical Review of Recurrent Neural Networks\n",
    "for Sequence Learning: [https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019)\n",
    "* A Convolutional Neural Network for Modelling Sentences: [https://arxiv.org/abs/1404.2188](https://arxiv.org/abs/1404.2188)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本教程以`LSTMEncoder`为例，展示如何用`paddlenlp.seq2vec`完成情感分析任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据加载\n",
    "\n",
    "ChnSenticorp数据集是公开中文情感分析数据集。PaddleNLP已经内置该数据集，一键即可加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在模型训练之前，需要先下载词汇表文件word_dict.txt，用于构造词-id映射关系。\n",
    "!wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "vocab = Vocab.load_vocabulary(\n",
    "    \"senta_word_dict.txt\", unk_token='[UNK]', pad_token='[PAD]')\n",
    "# Loads dataset.\n",
    "train_ds, dev_ds, test_ds = load_dataset(\n",
    "    \"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "for data in train_ds.data[:5]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句评论和对应的标签，0或1。0代表负向评论，1代表正向评论。   \n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "PaddleNLP提供了许多关于NLP任务中构建有效的数据pipeline的常用API\n",
    "\n",
    "| API                             | 简介                                       |\n",
    "| ------------------------------- | :----------------------------------------- |\n",
    "| `paddlenlp.data.Stack`          | 堆叠N个具有相同shape的输入数据来构建一个batch |\n",
    "| `paddlenlp.data.Pad`            | 将长度不同的多个句子padding到统一长度，取N个输入数据中的最大长度 |\n",
    "| `paddlenlp.data.Tuple`          | 将多个batchify函数包装在一起 |\n",
    "\n",
    "更多数据处理操作详见： https://github.com/PaddlePaddle/models/blob/release/2.0-beta/PaddleNLP/docs/data.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print(\"Stacked Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print(\"Padded Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "data = [\n",
    "        [[1, 2, 3, 4], [1]],\n",
    "        [[5, 6, 7], [0]],\n",
    "        [[8, 9], [1]],\n",
    "       ]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print(\"ids: \\n\", ids)\n",
    "print()\n",
    "print(\"labels: \\n\", labels)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本教程将对数据作以下处理：\n",
    "\n",
    "* 将原始数据处理成模型可以读入的格式。首先使用jieba切词，之后将jieba切完后的单词映射词表中单词id。\n",
    "\n",
    "* 使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple\n",
    "from utils import create_dataloader,convert_example\n",
    "\n",
    "# Reads data and generates mini-batches.\n",
    "tokenizer = JiebaTokenizer(vocab)\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, is_test=False)\n",
    "\n",
    "# 将读入的数据batch化处理，便于模型batch化运算。\n",
    "# batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。\n",
    "# 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.\n",
    "\n",
    "batch_size = 64\n",
    "use_gpu = True\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # input_ids\n",
    "    Stack(dtype=\"int64\"),  # seq len\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "train_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=batch_size,\n",
    "    mode='train',\n",
    "    use_gpu=use_gpu,\n",
    "    batchify_fn=batchify_fn)\n",
    "dev_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=batch_size,\n",
    "    mode='validation',\n",
    "    use_gpu=use_gpu,\n",
    "    batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型搭建\n",
    "\n",
    "使用`LSTMencoder`搭建一个BiLSTM模型用于文本分类任务。\n",
    "\n",
    "- `paddle.nn.Embedding`组建word-embedding层\n",
    "- `ppnlp.seq2vec.LSTMEncoder`组建句子建模层\n",
    "- `paddle.nn.Linear`构造二分类器\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecf309c20e5347399c55f1e067821daa088842fa46ad49be90de4933753cd3cf\" width = \"800\" height = \"450\"  hspace='10'/> <br />\n",
    "</p><br><center>图7：seq2vec详细示意</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "\n",
    "        # 首先将输入word id 查表后映射成 word embedding\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        # 将word embedding经过LSTMEncoder变换到文本语义表征空间中\n",
    "        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        # LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "\n",
    "        # 最后的分类器\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "\n",
    "\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        \n",
    "        # probs 分类概率值\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        return probs\n",
    "\n",
    "model= LSTMModel(\n",
    "        len(vocab),\n",
    "        len(train_ds.label_list),\n",
    "        direction='bidirectional',\n",
    "        padding_idx=vocab['[PAD]'])\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- `LSTMEncoder`参数：\n",
    "\n",
    "* `input_size`: int，必选。输入特征Tensor的最后一维维度。     \n",
    "* `hidden_size`: int，必选。lstm运算的hidden size。   \n",
    "* `num_layers`:int，可选，lstm层数，默认为1。    \n",
    "* `direction`: str，可选，lstm运算方向，可选forward， bidirectional。默认forward。\n",
    "* `dropout`: float，可选，dropout概率值。如果设置非0，则将对每一层lstm输出做dropout操作。默认为0.0。\n",
    "* `pooling_type`: str， 可选，默认为None。可选sum，max，mean。如`pooling_type=None`， 则将最后一层lstm的最后一个step hidden输出作为文本语义表征; 如`pooling_type!=None`， 则将最后一层lstm的所有step的hidden输出做指定pooling操作，其结果作为文本语义表征。\n",
    "   \n",
    "   \n",
    "更多`seq2vec`信息参考：[https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py](https://github.com/PaddlePaddle/models/blob/develop/PaddleNLP/paddlenlp/seq2vec/encoder.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 构造优化器，接入评价指标\n",
    "- 调用`model.prepare`配置模型，如损失函数、优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(\n",
    "        parameters=model.parameters(), learning_rate=5e-5)\n",
    "\n",
    "loss = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "model.prepare(optimizer, loss, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型训练与评估\n",
    "\n",
    "调用`model.fit()`一键训练模型。\n",
    "\n",
    "- **参数：**\n",
    "\n",
    "* `train_data` (`Dataset`|`DataLoader`) - 一个可迭代的数据源，推荐给定一个 `paddle.io.Dataset` 或 `paddle.io.Dataloader` 的实例。默认值：None。\n",
    "\n",
    "* `eval_data` (`Dataset`|`DataLoader`) - 一个可迭代的数据源，推荐给定一个 `paddle.io.Dataset` 或 `paddle.io.Dataloader` 的实例。当给定时，会在每个 epoch 后都会进行评估。默认值：None。\n",
    "\n",
    "* `epochs` (`int`) - 训练的轮数。默认值：1。\n",
    "\n",
    "* `save_dir` (`str`|`None`) - 保存模型的文件夹，如果不设定，将不保存模型。默认值：None。\n",
    "\n",
    "* `save_freq` (`int`) - 保存模型的频率，多少个 epoch 保存一次模型。默认值：1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_loader, dev_loader, epochs=10, save_dir='./checkpoints',  save_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这个非常基础的模型达到了90%的正确率，可以试试改变网络结构，进一步提升模型效果呦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型预测\n",
    "\n",
    "- 调用`model.predict`进行预测。\n",
    "\n",
    "- 参数\n",
    "* `test_data` (`Dataset`|`DataLoader`): 一个可迭代的数据源，推荐给定一个`paddle.io.Dataset` 或 `paddle.io.Dataloader` 的实例。默认值：None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, is_test=True)\n",
    "\n",
    "# 将读入的数据batch化处理，便于模型batch化运算。\n",
    "# batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。\n",
    "# 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.\n",
    "\n",
    "batch_size = 64\n",
    "use_gpu = True\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # input_ids\n",
    "    Stack(dtype=\"int64\"),  # seq len\n",
    "    Stack(dtype=\"int64\"), # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=batch_size,\n",
    "    mode='test',\n",
    "    use_gpu=use_gpu,\n",
    "    batchify_fn=batchify_fn)\n",
    "results = model.predict(test_loader, batch_size=64)[0]\n",
    "predictions = []\n",
    "for batch_probs in results:\n",
    "    # 映射分类label\n",
    "    idx = np.argmax(batch_probs, axis=-1)\n",
    "    idx = idx.tolist()\n",
    "    labels = [label_map[i] for i in idx]\n",
    "    predictions.extend(labels)\n",
    "\n",
    "# 看看预测数据前5个样例分类结果\n",
    "for idx, data in enumerate(test_ds.data[:5]):\n",
    "    print('Data: {} \\t Label: {}'.format(data['text'], predictions[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上简单介绍了基于LSTM的情感分类。可前往GitHub获取更多PaddleNLP的tutorial：[https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/rnn](https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/examples/text_classification/rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PaddleNLP 更多项目\n",
    "\n",
    " - [如何通过预训练模型Fine-tune下游任务](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    " - [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    " - [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    " - [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    " - [使用预训练模型ERNIE-GEN实现智能写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    " - [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    " - [使用预训练模型完成阅读理解](https://aistudio.baidu.com/aistudio/projectdetail/1339612)\n",
    " - [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
